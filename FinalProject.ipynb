{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozRa8yYOjNZr"
      },
      "source": [
        "#******************IMPORTS AND INSTALLATIONS******************\n",
        "!pip install lifetimes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "\n",
        "from lifetimes.utils import summary_data_from_transaction_data\n",
        "\n",
        "#load the data\n",
        "portfolio = pd.read_json('portfolio.json', lines=True)\n",
        "profile = pd.read_json('profile.json', lines=True)\n",
        "transcript = pd.read_json('transcript.json', lines=True)\n",
        "\n",
        "#******************EXPLORE PORTFOLIO DATASET******************\n",
        "print(portfolio.shape)\n",
        "print(portfolio.info())\n",
        "portfolio.head()\n",
        "\n",
        "\n",
        "#******************EXPLORE PROFILE DATASET******************\n",
        "print(profile.shape)\n",
        "print(profile.info())\n",
        "profile.head()\n",
        "\n",
        "# fill in missing values\n",
        "print(\"Number of missing values: \")\n",
        "print(profile.isnull().sum())\n",
        "\n",
        "# change the member time to day time\n",
        "profile['became_member_on'] = pd.to_datetime(profile['became_member_on'], format='%Y%m%d')\n",
        "\n",
        "# replace missing categorical values\n",
        "# fill empty genders\n",
        "profile['gender'] = profile['gender'].fillna('NA')\n",
        "print(profile['gender'].value_counts())\n",
        "\n",
        "print('\\nDescriptive stats for age and income:')\n",
        "print(profile.describe())\n",
        "\n",
        "# visualize the income data to understand which method to use to fill in missing values\n",
        "print(\"\\nHistogram of the income feature: \")\n",
        "plt.hist(profile['income'])\n",
        "plt.show()\n",
        "\n",
        "#non-longtail distribution = replace with mean\n",
        "\n",
        "#replace missing numerical values with mean values\n",
        "profile['income_na'] = profile['income'].isna().astype(int)\n",
        "profile[\"income\"] = profile[\"income\"].fillna(profile[\"income\"].mean())\n",
        "\n",
        "\n",
        "\n",
        "#******************EXPLORE TRANSCRIPT DATASET******************\n",
        "print(transcript.shape)\n",
        "print(transcript.info())\n",
        "transcript.head()\n",
        "\n",
        "# look at different types of events\n",
        "print(\"Unique events: \")\n",
        "print(transcript['event'].unique())\n",
        "\n",
        "# map it onto a histogram\n",
        "plt.hist(transcript['event'].unique())\n",
        "plt.show()\n",
        "\n",
        "# look at different statistics for time\n",
        "#print(\"Time data:\"\")\n",
        "#print(transcript.describe())\n",
        "\n",
        "\n",
        "#******************ONE HOT ENCODING******************\n",
        "# use dummies as one hot encoding method\n",
        "\n",
        "# use multi label binarizer\n",
        "binarizer = MultiLabelBinarizer()\n",
        "\n",
        "# use fit_transform() to fit the label sets binarizer and transform the given label sets\n",
        "channel_dummies = pd.DataFrame(binarizer.fit_transform(portfolio['channels']), columns= binarizer.classes_, index=portfolio.index)\n",
        "\n",
        "offer_df = portfolio['offer_type'].str.get_dummies()\n",
        "\n",
        "gender_df = profile['gender'].str.get_dummies().add_prefix('gender_')\n",
        "\n",
        "profile['year_joined'] = profile['became_member_on'].apply(lambda x: str(x.year))\n",
        "\n",
        "year_df = profile['year_joined'].str.get_dummies().add_prefix('year_joined_')\n",
        "\n",
        "events_df = transcript['event'].str.get_dummies()\n",
        "events_df.drop('transaction', axis=1, inplace=True)\n",
        "\n",
        "#create a new data frame for portfolio including potrfolio, channel dummies, and offer dummies\n",
        "portfolio = pd.concat([portfolio, channel_dummies, offer_df], axis=1)\n",
        "print(portfolio.info())\n",
        "portfolio.head()\n",
        "\n",
        "#create a new data frame for profile, including profile, gender dummies and and year joined dummies\n",
        "profile = pd.concat([profile, gender_df, year_df], axis=1)\n",
        "\n",
        "profile.drop(['became_member_on'], axis=1, inplace=True)\n",
        "profile.drop(['year_joined'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#create a new data frame for transcript, including transcript and type of event dummies\n",
        "transcript = pd.concat([transcript, events_df], axis=1)\n",
        "\n",
        "# print them out\n",
        "profile.head()\n",
        "portfolio.head()\n",
        "transcript.head()\n",
        "\n",
        "#******************MERGING DATASETS ******************\n",
        "\n",
        "# for offers = all the events that are not transactions\n",
        "offers_data = transcript.query('event != \"transaction\"').copy()\n",
        "offers_data['offer_id'] = offers_data['value'].apply(lambda x: list(x.values())[0])\n",
        "offers_data.drop(['value'], axis=1, inplace=True)\n",
        "\n",
        "#merge with the profile and portfolio\n",
        "offers_data = offers_data.merge(profile, left_on='person', right_on='id')\n",
        "offers_data = offers_data.merge(portfolio, left_on='offer_id', right_on='id')\n",
        "offers_data.drop(['id_x', 'id_y'], axis=1, inplace=True)\n",
        "offers_data.set_index('offer_id', inplace=True)\n",
        "\n",
        "# remove the outliars that are over the age of 99\n",
        "offers_data = offers_data.query('age <= 99')\n",
        "\n",
        "# for transactions = events that resulted in transactions\n",
        "transac_data = transcript.query('event == \"transaction\"').copy()\n",
        "transac_data['amount'] = transac_data['value'].apply(lambda x: list(x.values())[0])\n",
        "transac_data.drop(['value', 'offer completed', 'offer received', 'offer viewed'], axis=1, inplace=True)\n",
        "\n",
        "#merge with profile\n",
        "transac_data = transac_data.merge(profile, left_on='person', right_on='id')\n",
        "transac_data.drop(['event'], axis=1, inplace=True)\n",
        "\n",
        "# remove the outliars that are over the age of 99\n",
        "transac_data = transac_data.query('age <= 99')\n",
        "\n",
        "# remove the outliars that are over the age of 99\n",
        "profile = profile.query('age <= 99')\n",
        "\n",
        "offers.reset_index(inplace=True)\n",
        "\n",
        "#visualize the data\n",
        "profile.head()\n",
        "print(profile.info())\n",
        "\n",
        "transac_data.head()\n",
        "print(transac_data.info())\n",
        "\n",
        "offers_data.head()\n",
        "print(offers_data.info())\n",
        "\n",
        "# see duplicate offers in the offers data\n",
        "\n",
        "\n",
        "#******************DEALING WITH DUPLICATE OFFERS******************\n",
        "offers_data[offers_data.duplicated(subset=['offer_id', 'person'], keep=False)].head()\n",
        "\n",
        "#******************ADD RECEIPT, VIEW AND COMPLETION TIMES******************\n",
        "# add 0.1 so there is no 0 values for time\n",
        "offers_data['time'] = offers_data['time'] + 0.1\n",
        "\n",
        "offers_data['received_time'] = offers_data['offer_received'] * offers_data['time']\n",
        "offers_data['viewed_time'] = offers_data['offer_viewed'] * offers_data['time']\n",
        "offers_data['completed_time'] = offers_data['offer_completed'] * offers_data['time']\n",
        "\n",
        "#visualize the data\n",
        "offers_data.head()\n",
        "\n",
        "#******************KEEPING TRACK OF MULTIPLE OFFERS BY THE SAME PERSON******************\n",
        "min_dupes = offers_data.groupby(['person', 'offer_id', 'event']).min()\n",
        "num_dupes = offers_data.groupby(['person', 'offer_id', 'event'])[['offer_received', 'offer_viewed', 'offer_completed']].sum()\n",
        "min_dupes.update(num_dupes)\n",
        "\n",
        "offers_grouped = min_dupes.groupby(['person', 'offer_id']).max().reset_index()\n",
        "offers_grouped.fillna(0, inplace=True)\n",
        "\n",
        "offers_grouped['end_time'] = offers_grouped['received_time'] + offers_grouped['duration'] * 24\n",
        "offers_grouped['viewed_on_time'] = offers_grouped.apply(lambda x: 1 if x['viewed_time'] < x['end_time'] and x['viewed_time'] != 0 else 0, axis=1)\n",
        "offers_grouped['completed_on_time'] = offers_grouped.apply(lambda x: 1 if x['completed_time'] < x['end_time'] and x['completed_time'] != 0 else 0, axis=1)\n",
        "\n",
        "offers_grouped = offers_grouped.query('~(completed_time > 0 and viewed_time == 0)')\n",
        "offers_grouped = offers_grouped.query('~(completed_time != 0 and completed_time < viewed_time)')\n",
        "offers_grouped = offers_grouped.query('~(viewed_time != 0 and viewed_time < received_time)')\n",
        "\n",
        "#******************CONVERTING TIME TO DATE TIME******************\n",
        "transac_data['datetime'] = pd.to_datetime(transac_data['datetime'], format='%Y%m%d')\n",
        "\n",
        "#******************FREQUENCY******************\n",
        "frequency = summary_data_from_transaction_data(transac_data, 'person', 'datetime', monetary_value_col='amount')\n",
        "frequency.drop('T', axis=1, inplace=True)\n",
        "\n",
        "profile.rename({'id': 'person'}, axis=1, inplace=True)\n",
        "profile.set_index('person', inplace=True)\n",
        "\n",
        "customer_info = profile.join(frequency)\n",
        "\n",
        "#print out the information about customers\n",
        "customer_info.head()\n",
        "print(customer_info.info())\n",
        "\n",
        "\n",
        "#useful functions\n",
        "def sum_greater_than_zero(x):\n",
        "    return (x > 0).sum()\n",
        "\n",
        "def mean_greater_than_zero(x):\n",
        "    return x.replace(0, np.nan).mean()\n",
        "\n",
        "#******************BOGOG OFFERS******************\n",
        "bogo_offers = offers_grouped.query('offer type == \"bogo\"').groupby(['person']).agg(received_count = ('offer received', sum_greater_than_zero),\n",
        "                                                                                   viewed_count = ('offer viewed', sum_greater_than_zero),\n",
        "                                                                                   completed_count = ('offer completed', sum_greater_than_zero))\n",
        "\n",
        "# calculate ratios of bogo offers that were viewed to offers received\n",
        "bogo_offers['bogo_view_rate'] = bogo_offers['viewed_count'] / bogo_offers['received_count']\n",
        "\n",
        "# calculate ratios of offers that were completed (= converted) to offers received\n",
        "bogo_offers['bogo_convert_rate'] = bogo_offers['completed_count'] / bogo_offers['viewed_count']\n",
        "\n",
        "bogo_offers.drop(['viewed_count', 'received_count', 'completed_count'], axis=1, inplace=True)\n",
        "\n",
        "#******************DISCOUNT OFFERS******************\n",
        "discount_offers = offers_grouped.query('offer type == \"discount\"').groupby(['person']).agg(received_count = ('offer received', sum_greater_than_zero),\n",
        "                                                                                                viewed_count = ('offer viewed', sum_greater_than_zero),\n",
        "                                                                                                completed_count = ('offer completed', sum_greater_than_zero),\n",
        "                                                                                               )\n",
        "# calculate ratios of discount offers that were viewed to offers received\n",
        "discount_offers['discount_view_rate'] = discount_offers['viewed_count'] / discount_offers['received_count']\n",
        "# calculate ratios of discount offers that were viewed to offers received\n",
        "discount_offers['discount_convert_rate'] = discount_offers['completed_count'] / discount_offers['viewed_count']\n",
        "\n",
        "discount_offers.drop(['viewed_count', 'received_count', 'completed_count'], axis=1, inplace=True)\n",
        "\n",
        "#******************INFORMATIONAL OFFERS******************\n",
        "info_offers = offers_grouped.query('offer type == \"informational\"').groupby(['person']).agg(received_count = ('offer received', sum_greater_than_zero),\n",
        "                                                                                            viewed_count = ('offer viewed', sum_greater_than_zero),\n",
        "                                                                                            completed_count = ('offer completed', sum_greater_than_zero),\n",
        "                                                                                           )\n",
        "# calculate ratios of informational offers that were viewed to offers received\n",
        "info_offers['informational_view_rate'] = info_offers['viewed_count'] / info_offers['received_count']\n",
        "# calculate ratios of informational offers that were completed to offers viewed\n",
        "info_offers['informational_convert_rate'] = info_offers['completed_count'] / info_offers['viewed_count']\n",
        "\n",
        "info_offers.drop(['viewed_count', 'received_count', 'completed_count'], axis=1, inplace=True)\n",
        "\n",
        "#******************ALL OFFERS******************\n",
        "all_offers = offers_grouped.query('offer type != \"informational\"').groupby(['person']).agg(\n",
        "    received_count = ('offer received', sum_greater_than_zero),\n",
        "    viewed_count = ('offer viewed', sum_greater_than_zero),\n",
        "    completed_count = ('offer completed', sum_greater_than_zero),\n",
        ")\n",
        "\n",
        "# calculate ratios of all offers that were viewed to offers received\n",
        "all_offers['total_view_rate'] = all_offers['viewed_count'] / all_offers['received_count']\n",
        "\n",
        "# calculate ratios of all offers that were completed to offers viewed\n",
        "all_offers['total_convert_rate'] = all_offers['completed_count'] / all_offers['viewed_count']\n",
        "\n",
        "all_offers.drop(['offers_viewed_cnt', 'offers_received_cnt', 'offers_completed_cnt'], axis=1, inplace=True)\n",
        "\n",
        "#join the data sets\n",
        "customer_performance = customer_performance.join(bogo_offers).join(discount_offers).join(info_offers).join(all_offers)\n",
        "customer_performance.fillna(0, inplace=True)\n",
        "\n",
        "customer_performance.head()\n",
        "print(customer_performance.info())\n",
        "\n",
        "\n",
        "#******************CUSTOMER OFFERS******************\n",
        "customer_offers = offers_grouped.groupby('person').agg(\n",
        "    mean_reward_amount = ('reward', 'mean'),\n",
        "    mean_difficulty = ('difficulty', 'mean'),\n",
        "    email_offers_count = ('email', 'sum'),\n",
        "    mobile_offers_count = ('mobile', 'sum'),\n",
        "    social_cnt = ('social', 'sum'),\n",
        "    web_offers_count = ('web', 'sum'),\n",
        "    bogo_offers_count = ('bogo', 'sum'),\n",
        "    discount_offers_count = ('discount', 'sum'),\n",
        "    info_offers_count = ('informational', 'sum'),\n",
        "    offers_received_count = ('offer received', sum_greater_than_zero),\n",
        "    offers_viewed_count = ('offer viewed', sum_greater_than_zero),\n",
        "    offers_completed_count = ('offer completed', sum_greater_than_zero),\n",
        "    mean_receive_amount = ('offer received', 'mean'),\n",
        "    mean_view_amount = ('offer viewed', 'mean'),\n",
        "    mean_complete_amount = ('offer completed', 'mean'),\n",
        ")\n",
        "\n",
        "#join data\n",
        "customer_performance = customer_performance.join(customer_offers)\n",
        "\n",
        "customer_performance.drop(['gender', 'gender_NA', 'income_na'], axis=1, inplace=True)\n",
        "customer_performance.dropna(inplace=True)\n",
        "\n",
        "#******************DATA EXPLORATION******************\n",
        "starbucks_colors = [\"tan\", \"grey\", \"orange\", \"navy\", \"purple\"]\n",
        "starbucks_d = [\"c\", \"m\", \"y\"]\n",
        "gender_colors = [\"b\", \"r\", \"g\"]\n",
        "sns.set_palette(sns.color_palette(starbucks_colors))\n",
        "\n",
        "#plot gender data\n",
        "df = transac_data[['gender', 'age', 'amount', 'income']]\n",
        "\n",
        "gen_plot = sns.countplot(x='gender', data=df)\n",
        "gen_plot.set_title('Gender data: ');\n",
        "sns.set_palette(sns.color_palette(starbucks_colors))\n",
        "\n",
        "#map gender female/male\n",
        "df = transac_data[['gender', 'age', 'amount', 'income']].query('gender in [\"F\", \"M\"]')\n",
        "df['gender'] = df['gender'].map({'M': 'Male', 'F': 'Female'})\n",
        "\n",
        "gen = sns.pairplot(df, hue='gender')\n",
        "gen.fig.suptitle(\"Gender data: age and income\", y=1.00);\n",
        "sns.set_palette(sns.color_palette(starbucks_colors))\n",
        "\n",
        "# plot offer by event type\n",
        "df = offers.groupby(['event', 'offer_type'])['person'].count().reset_index()\n",
        "df['event'] = pd.Categorical(df['event'], [\"offer received\", \"offer viewed\", \"offer completed\"])\n",
        "df.sort_values('event', inplace=True)\n",
        "\n",
        "axis1, axis2 = plt.subplots(ncols=3, figsize=(20, 4))\n",
        "\n",
        "df['event'] = df['event'].map({'offer received': 'Received', 'offer viewed': 'Viewed', 'offer completed': 'Completed'})\n",
        "\n",
        "ax = sns.barplot('event', y='person', data=df.query('offer_type == \"bogo\"'), palette=starbucks_d, ci=None, ax=axis2[0])\n",
        "ax.set(xlabel='', ylabel='Count', title='BOGO')\n",
        "\n",
        "ax = sns.barplot('event', y='person', data=df.query('offer_type == \"discount\"'), palette=starbucks_d, ci=None, ax=axis2[1])\n",
        "ax.set(xlabel='', ylabel='Count', title='Discount')\n",
        "\n",
        "ax = sns.barplot('event', y='person', data=df.query('offer_type == \"informational\"'), palette=starbucks_d, ci=None, ax=axis2[2])\n",
        "ax.set(xlabel='', ylabel='Count', title='Informational')\n",
        "\n",
        "axis1.suptitle('Types of Offers', size=15, y=1.05);\n",
        "\n",
        "#******************FEATURE SCALING******************\n",
        "scaler = StandardScaler().fit(customer_performance)\n",
        "customers_scaled = scaler.transform(customer_performance)\n",
        "customers_scaled.std(axis=0)[:5], customers_scaled.mean(axis=0)[:5]\n",
        "\n",
        "#******************DIMENSIONALITY REDUCTION******************\n",
        "pca = PCA()\n",
        "\n",
        "X_pca = pca.fit_transform(customers_scaled)\n",
        "\n",
        "num_components=len(pca.explained_variance_ratio_)\n",
        "ind = np.arange(num_components)\n",
        "vals = pca.explained_variance_ratio_\n",
        "cumvals = np.cumsum(vals)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "#bar plot\n",
        "ax = sns.barplot(ind, vals, palette=starbucks[:1], ci=None)\n",
        "\n",
        "#line plot\n",
        "ax2 = sns.lineplot(ind, cumvals, color=starbucks[1], ci=None)\n",
        "ax.grid(b=True, which='major', linewidth=0.5)\n",
        "\n",
        "ax.set_xlabel(\"Principal component\")\n",
        "ax.set_ylabel(\"Variance explained (%)\")\n",
        "plt.title('Explained variance per principal component');\n",
        "cum_expl_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components = len(cum_expl_var_ratio[cum_expl_var_ratio <= 0.8])\n",
        "\n",
        "pca = PCA(num_components).fit(customers_scaled)\n",
        "X_pca = pca.transform(customers_scaled)\n",
        "X_pca = pd.DataFrame(X_pca)\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "#******************KMEANS CLUSTERING******************\n",
        "cluster_model = KMeans(n_clusters=5, random_state=28).fit(X_pca)\n",
        "kmeans_clusters = cluster_model.predict(X_pca)\n",
        "df = customers.copy().reset_index()\n",
        "df['cluster'] = kmeans_clusters\n",
        "df = df.melt(id_vars=['person', 'cluster'])\n",
        "ax = sns.countplot(x='cluster', data=df)\n",
        "ax.set_title('Customers by cluster');"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}